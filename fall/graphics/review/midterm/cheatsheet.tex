\documentclass[portrait, 10pt, a4paper]{article}
\usepackage[margin=0.3in]{geometry}
\usepackage{multicol}
\usepackage{mathtools}
\title{Introduction to Computer Graphics}
\begin{document}
\raggedright
\begin{multicols*}{3}
\section{1. Introduction}
	\subsection{History}
		In the 1950s and 1060s, the cost for CRTs was too high, since the computers were too expensive and slow.
		Mah boy Cohen Sutherland developed many of the algos responsible fro todays graphics.
		
		The idea of a \textbf{raster}, or an array of pixels, came along as a way to represent images.

		In the 80s and 90s, there came more hardware to do some of the calcualtions required.
	\subsection{Image Formation}
		In an image, there are \textbf{objects}, \textbf{viewers}, and \textbf{light sources}. Different attributes
		govern how all of these elements interact in common. 

		We can have rays of light come out of the light source. \textbf{Light} is a part of the EM spectrum that us
		humans can perceive with our light. The technique called \textbf{ray-tracing} involves following an individual
		light source and then attempts to figure out whether it enters the camera. The issue with this approach is that
		a single light source can have multiple interactions before entering the lens of the camera, so figuring out
		if it made it into the scene will take a lot of processing power for all the light rays.
	\subsection{Color}
		The prevailing theory of how humans perceive color is through \textit{three-color theory}, which states that 
		humans have two types of receptors in our eyes that together capture. There is \textit{additive color} and
		\textit{subtractive color}, which means that 
	\subsection{camera}
		When talking about a camera, we need several things: the \textit{image plane}, where the image is being 
		projected onto a plane, the \textit{projector}, which directs the iamge from the image plane to the 
		center of projection. With a simple \textbf{pinhole camera}, the image essentially gets flipped upside 
		down after entering the peephole, since the lens restricts the light coming in. The perspective equations
		are:
		\[ x_{p}=-x(z/d)\]
		\[ y_{p}=-y/(z/d)\]
		\[ z_{p}=d\]
		Remember the perspective projection uses similar triangles! Essentially, we have to pass from the original
		clipping plane to the plane encompassing $(1,1,1) - (-1,-1,-1)$. These formulas are how we apply the 
		transformations.
	\subsection{image representation}
		Because images are hard to compute, instead we use \textbf{primitives}, which allow us  to break down an 
		arbitrarily complex figure into a collection of simpler polygons. Since triangles are the simplest polygons, 
		the hardware is usually specially designed to deal with polygons. After we draw the polygons, we need to remove
		\textit{hidden surfaces}.
\section{Clipping}
		(I cover this out of order because I had not studied this).
		\subsection{Objectives}
			For clipping, we want to make sure we know what objects make it into the scene. Once we know what things 
			make it into the scene, we need to know what objects are in front of others. We need to know clipping, 
			which is easy for lines and polygons, but harder for other types of figures. 
		\subsection{2d Clipping}
			If we were using the brute force approach, we would calculate all the intersections with the sides of the
			viewing rectangle.
		\subsection{Cohen-Sutherland Algorithm}
			Without calculating intersections, we can eliminate some of the lines. The most important
			thing is to calculate the window. We then have a couple cases:
			\begin{itemize}
				\item{Case 1:} Both endpoints of a line segment are within the window. Accept the line.
				\item{Case 2:} Both endpoints lie on the outside and on the same side of the line(they don't
								come in to the window). Reject the segment.
				\item{Case 3:} One endpoint inside and one outside. Here we have to perform some calculations.
				\item{Case 4:} If they are both outside, we might have to do some calculations if they have one 
								part inside.
			\end{itemize}
		\subsection{Outcodes}	
			Because we have four pairs of intersecting lines, we need to calculate 9 outcodes, which we need 4 
			bits for. We will need at most four subtractions to calculate the intersection. The initial point $C$
			and endpoint $D$ of a line-segment will each land on a segment. We compare the subtraction of the 
			two endpoints to see if they both lie on the same section. 

			If the logical ANDing of the two outcodes does not equal 0, then we know that the segment lies outside
			the region and we reject it. Else if the the logical AND yields 0 and the segments are not the same, 
			we will need to calculate the intersection.
		\subsection{efficiency}
			Usually the efficiency is not bad since the clipping window is usually relatively small to the size of the
			database.
		\subsection{Polygons}
			\textit{Convex} polygons can only create one other shape when passed through this algorithm. If the polygon
			is non-convex, then we need to \textit{tesselate} it, or split it into smaller triangles.
		\subsection{Pipelining}
			Since the intersections on the sides can be calculated independently, we can insted create a separate 
			pipeline accross the sides of the window. The \textit{Sutherland-Hodgman} algorithm can detect the overlapping
			segments of a non-convex polygon by extending the sides of the clipping polygons indefinitely. The 
			Cohen-Sutherland algorithm can be used in 3d, however it requires 6-bit outcodes instead of 4.
\section{Programmable Shaders}
		Essentially, since the first development of GPUs, after a while we were able to develop programmable shaders. 
		This means we can customize per-vertex functions such as Phong shading, colors, normal transformation, etc...

		What is a \textit{fragment}? ``potential pixel". We can perform per-fragment calculations without knowledge 
		about other fragments.

		\subsection{D3D 10 Pipeline}
		\begin{itemize}
			\item{Input assembler}: Receives the sets of coordinates to the pipeline.
			\item{Vertex Shader}: does vertices, transformation, skinning, lighting
		\end{itemize}
		...etc
\section{Shading}
	\subsection{Illumination} We have different sources of light/shading, and different surface properties. 
	\subsection{Light-Matieral Interaction}
		Reflection of light depends the surface of the object. The \textit{rendering equation} works over all the 
		space, since we have  see how the light reflections affect a specific region, however this is impractical
		for real-time usage. \textbf{Local Illumination} refers to the lighting of just the specific vertices that

	\subsection{Light Sources}
		Light sources are complicated b/c they emit light for more than one point. Then, more than one light ray from
		a source would reach the source, and we would have to integrate over the entire sruface.
	\subsection{Light Sources}
		\begin{itemize}
			\item{Point source}:There is just a point that emits color in all directions. Infinite distance lights 
				would mean the light rays coming to the scene are parallel.
			\item{Spotlight}:There is a cone which emits light in a restricted area.
			\item{Ambient Light}: Ambient light would generate equal lighting all over the scene.
		\end{itemize}
		The smoother a surface, the more the light will be reflected off of its surface. Because a rough surface 
		will reflect rays more erratically, the light will reflect in all directions.
	\subsection{Phong Reflection Model}
		The reflection model used by Phong keeps four vectors: the vector to the source $l$, the vector to the viewer
		$v$, the normal $n$, and the vector assuming perfect reflection $r$. 

		The three components of Phong Lighting are: Ambient, Diffuse, Specular. \textit{Diffuse} light scatters light
		in all directions. The intensity of the light will depend on the angle of reflection, since it would be 
		spread over a larger area.

		\textit{Specular} surfaces reflect light close to the region of a perfect reflection. $l_{r}k_{s}l\cos^{a}\phi$, 
		which relates reflected intensity, absorption coeff, incoming intensity, and shininess coefficient. The shininess
		coefficient can change the texture of a material. Higher values correspond to more metallic looks.

		We also have to add a term for the distance from a source, which is inversely proportional to the distance.
		
		The modified Phong Model addresses the problem of using the complicated complication, so instead we use 
		the \textit{halfway angle}.
	\subsection{Computation of Vectors}
		$I$ and $v$, which are specified by the application. $r$, which is computed from $l$ and $n$.
	\subsection{Shading}
		How do we fill color within a polygon? Flat shading, Gouraud shading, or Phong Shading.

		\textit{Flat Shading} assumes that the vectors are constant.

		\textit{Gouraud Shading} finds the average noraml at each vertex. However, this can lead to \textit{Mach Bands}, 
		becuase the contrast between the planes is still sharp. Take the normal of the vertex and find the value 
		corresponding it, and apply it to the planes.

		\textit{Phong Shading} finds the vertex normal, and interpolates vertex normals, and we find shades along the 
		edges.
\end{multicols*}
\end{document}
