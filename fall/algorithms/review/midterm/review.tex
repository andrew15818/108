
\documentclass[portrait,10pt, a4paper]{article}
\usepackage[margin=0.3in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{multicol}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{ifthen}

\begin{document}
\raggedright
\begin{multicols*}{3}
\section{Introduction}
\subsection{What is an algorithm?}
Formally, an algorithm is just a procedure to turn one form of input into another one.

\subsection{The champion problem}
		Given a set of n elements, how do we select the minimum or the maximum ones? The \textit{naive}
		solution involves performing a linear scan on all the items in the array. However, for each item in the 
		array, we would need to perform some amount of computation. Thus, the time it takes to solve the problem
		would scale as the size of the input scales up.

\subsection{Selection Sort}	
		Given a set of n elements, output the entire sequence in non-decreasing order. Using the champion problem,
		we see that that we can \textit{reduce} the sorting problem to the champion problem. We pull the min number
		from the unsorted section of the array, and insert it into the sorted prefix array. Since we pull the minimum
		number from $n$, then $n-1$ elements, then we will pull the elements in a non-decreasing order.

		\begin{tikzpicture}
				\filldraw[red](-1,0) circle (2pt) node[anchor=north]{1};
				\draw[black, thin] (-1,0) -- (-.1,0);
				\filldraw[blue](0,0) circle (2pt) node[anchor=north]{2};
				\draw[black, thin] (0,0) -- (.9, 0);
				\filldraw[blue] (1,0) circle (2pt) node[anchor=north]{3};
		\end{tikzpicture}

		During the first call to $\text{champion}(i,n)$, we get the smallest value and swap it with the first item of a 
		sorted array. Repeating this process eventually gives us the correct answer.
\subsection{Insertion Sort}
		This is similar to the way we sort a deck of cards. For card $i$, we move from position $i$, until we find the 
		previous item that is less than the number.
\subsection{Asymptotic notation}
		When we talk about how many calculations some operation takes, we mean to say \textit{how the number of calculations		} scale with the size of the input. We can assign a formula for the amount of calculations, then we take the leading
		exponent, and call  it the complexity. We classify it into several portions:

		\[ f(n) \leq C \times g(n) \forall n \geq n_{0}\]
		
		Thus, if a function $f(n)$ is $O(n)$, then $f(n)$ varies from $g(n)$ by no less than a constant factor. This makes
		it easier to discuss the \textit{time-complexity}.
\subsection{Merge Sort}
		The main idea is that we have two different arrays, and we want to just compare item $j$ and item $i$ for every
		one in the two arrays. Using this way, we then recursively sort the two halves of each subarray until we hget all
		the base cases. This then gives us an algo with $O(n\log(n))$.

\subsection{Recurrence Relations}
		We can describe the way how the instances realte to previoius instances of the problem. There are several methods:
		The substitution method, where we unroll the loop; the Recursion-tree method, where we guess an draw the 
		recursion tree method; the final one is the Master Theorem.
\section{Lower Bounds}
		\subsection{Membership Query Problem}	
			Basically, we want to know whether for a given array x with $n$ elements, there exists an $x$ and $i$ such 	
			that $A[i] = x$. The naive solution involves a \textit{linear scan}. If the array is sorted, then we can 
			sort the array in $O\log(n)$ with \textit{Binary Search}. Within the comparison-based model, the ordering
			between the elements can only be determined with comparisons.
		\subsection{More Asymptotic Notation}
			After we learned $O(n)$, we define $\Omega(n)$ and $\Theta(n)$, which act as a lower and upper bound,
			respectively.

			After that, we introduce $o(n)$ and $\omega(n)$, which are defined as:
			\[ \lim_{x\to\infty}\frac{f(n)}{g(n)} = 0\] and 
			\[ \lim_{x\to\infty}\frac{f(n)}{g(n)} = \infty\]
				
			You want to remember: \textbf{if $f(n)=o(g(n)),\text{then} f(n)=O(g(n))$, but not-vice-versa}. Use L'Hopital's
			rule to show this is sometimes easier to do.

			\textbf{asymptotically optimal:} When an algorithm runs in $\Omega(f(n))$ and $O(f(n))$, we know that the 
			algorithm can't be improved by a \textit{superconstant} factor.
		\subsection{Sorting Lower Bound}
			So far, we have a limit of $\Omega(n\log(n))$ for the comparison-based model, becuase both its upper bound and
			lower bound estimates are the same. If we can choose one of the $n!$ permutations, we shold then ask the 
			evil adversary to remove  a certain number of the possible permutations based on the ordering of the elements.
			We can show that $n\log(n)$ is asymptotically optimal.
\section{Divide and Conquer}
	\subsection{What exactly is D\&C?}
		D\&C  involves recursion, or solving the same problem for a smaller instance of the problem. The idea is 
		that to solve a problem of size $n$, we need the solutions to smaller instances of the problem and then 
		somehow merge them together (i.e. the conquer step). In the mergesort this would be merging the two
		arrays after they have been sorted for lower instances of the problem.
	\subsection{Quick Sort}
		This algo chooses a \textit{pivot} from the array (different ways of choosing it), then creates two arrays
		, $S$ and $L$. In $S$ we place all the elements less than $k$(our pivot), and in $L$ the elements larger.
		We then recursively call the function on these two arrays.

		What is the running time? It depends on the relative sizes of S and L. If either is always empty, then
		we will just be recursing on $(n-1)$ every time, which will lead to $O(n^{2})$. If $|S|-|L|<1$, then our 
		recursion is \textit{balanced} and thus reaches the desired $O(n\log(n))$ time.

		However, for a arbitrary ratio  $|S|:|L|$, the worst-case is $(n,n-1,...,1)$, so $O(n^{2})$
	\subsection{Randomized Quick Sort}
		Random selections of pivots usually result in balanced trees. Why? By the \textit{Chernov Inquality}, we
		know that a bounded portion of the values will be a larger distance away from the mean.
	\subsection{Selection}
		AKA the $k-th$ order statistics, we seek to find the index of the k-th smallest number. We can split
		the arrays into $S$ and $L$ again, and if 
\section{Linear-Time Algorithms}
	\subsection{Seelction}
		The naive way to find the min element is to do a linear scan. How to find the order statistic in linear 
		time? It depends on the selection of a good pivot. 
\section{Geometric Problems}
	\subsection{Bichromatic Planar Matching}
		Given  a set of $n$ points on a plane, we want to find a pairing of points such that we can draw lines 
		to all of them without them crossing.

		\begin{tikzpicture}
				\draw[black, thin] (-1,0) -- (0,-1);
				\draw[black, thin] (0,1) -- (1,0);
				\filldraw[blue] (0,1) circle (4pt) ;
				\filldraw[blue] (-1,0) circle (4pt);
				\filldraw[red] (1,0) circle (4pt);	
				\filldraw[red] (0,-1) circle (4pt) node[anchor=north]{valid pairing};	
		\end{tikzpicture}
			
		Notice how neither line crosses when connecting to the other points. Generalizing to $n$ points, no lines
		should intersect when trying to find one such pairing. A possible solution: assign an arbitrary 
		arrangement and individually fix it.
	\subsection{Line Segment Intersection}
		To decide whether two line segments intersect, we need to check for the direction between two different
		line segments, whether we need a \textit{clockwise rotation} or \textit{counter-clockwise rotation}.
		\begin{equation}
				\left(\begin{array}{cc} x(q)-x(a) & y(q)-y(a) \\x(b)-x(a) & y(b)-y(a)\end{array}\right)
		\end{equation}
		Here, $q$ is the endpoint of the vector we're measuring $\Theta$ from, $a$ is the point
		where the two vectors intersect, and $b$ is the other vector. If the result of the calculation is >0,
		it is in the right halfplane, <0 is in the left halfplane, and 0 means colinear. 
		
	\subsection{Ham-Sandwich Cut}
		Given a set of blue points and red points on a plane, draw a straight line so that we can split the
		region into two containing equal amounts of red and blue points. Takes $O(n+m)$ time.
		
		With BMP, we can solve the HSC by joining together two points on opposite sides of the splitting line,
		or to each other if there is just one point on the area. This way we can get a pairing og red and blue
		points whose lines do not intersect in $O(n\log(n))$.
	
	\subsection{closest pair of points}
		Given S points, find the shortest distance among any of the two points.

		We have three scenarios: the shortest distance might lie on the left or right section, otherwise one 
		of the endpoints might be across the side from the other.

		We then take the min of the closest point from the two sides, and to check for the center one, we 
		calculate the distance $\delta$ from the dividing line. The closest pair can be at most $2\delta$ away
		if it is to be the closest point.
	\subsection{Convex Hulls}
		The convex hull of a ste of points is the smallest polygon which can contains the given input set.
		We can know that the points with the largest and smallest x coordinates will be on the convex hull.
		Once we find those, we find the longest point $L$ whose perpendicular distance to PQ is the longest.
		We do this for the line segment PQ and QP. We iterate until no new points can be found. This algorithm
		will run in $O(nh)$.
	\subsection{Farthest Pair}
		Given a set of points on the plane, output the longest distance between any two points. The two points
		must be on opposite ends of the convex hull. If they were both inside, then we could draw them out a bit
		farther and still be within the covnex hull. Therefore, the longest distance has to be held by two 
		points who lie on the convex hull. This has a worst-case running time of $O(n^{2})$, when all the points
		lie on the convex hull. How do we fix it?
	\subsection{antipodal pair}

\end{multicols*}
\end{document}
