\documentclass{article}
%necessary packages, use xelatex to compile
\usepackage[margin=1in]{geometry}
\usepackage{helvet}
\usepackage{xeCJK}
\usepackage{amsmath}
%changing the main font
\renewcommand{\familydefault}{\sfdefault}
%cool traditional chinese font for my name :^D
\setCJKmainfont{Noto Serif CJK TC}
\title{Introduction to Pattern Recognition Homework 2 Report}
\author{Andr\'es Ponce(彭思安) \\
\and
0616110}
\begin{document}

\maketitle
\section{Coding}
	\subsection{Compute the mean vectors $m_{i},(i=1,2)$ of each 2 classes on training data}
		To calculate the \textbf{mean vectors} for each class, we need to know which of the 
		input vectors belong to $C_{1}$ and $C_{2}$, respectively. This is done by just checking
		which rows in \texttt{y\_test} are 0 and 1, since those tell us which class the test input
		vectors belong to. After separating them and getting the mean for each column separately,
		we end with the result
		\[ m_{1} = [2.47107265 \textrm{ } 1.97913899]\quad m_{2} = [1.82380675 \textrm{ } 3.03051876]\]
	\subsection{Compute the within-class scatter matrix $S_{W}$ on training data.}
		The within class matrix $S_{W}$ measures how much the points in ever class differ from the mean.
		The within class variance for the dataset is the sum of the within-class variance for all the classes
		in our dataset. The result is a $2x2$ matrix with the values:

		\begin{equation*}
			S_{W} = 
			\begin{bmatrix}
				140.40035447 & -5.30881553\\
				-5.30881553 & 138.14297637
			\end{bmatrix}
		\end{equation*}
	\subsection{Compute the between-class scatter matrix $S_{B}$ on training data.}
		The between-class covariance matrix $S_{B}$ measures how much the two class means
		vary from each other. The result is another $2x2$ matrix:

		\begin{equation*}
			S_{B} = 
			\begin{bmatrix}
				0.41895314 & -0.68052227\\
				-0.68052227 &  1.10539942
			\end{bmatrix}
		\end{equation*}
	\subsection{Compute the Fisher's linear discriminant $W$ on training data.}
		The Fihser's Linear Discriminant can be calculated by the ratio of the between-class
		variance to the within-class variance. The linear discriminant comes out to be 
		\begin{equation*}
			w = 
			\begin{bmatrix}
				-0.00432865 \\
				0.00744446
			\end{bmatrix}
		\end{equation*}
	\subsection{Project the testing data by linear discriminant to get the class prediction by
		nearest-neighbor rule and calculate your accuracy score on testing data.}
		The projection of the test data can be done once we have calculated the $w$ on the training
		data. For this assighment, we can call \texttt{accuracy\_score(...)} from the \texttt{sklearn}
		module. 
	\subsection{Plot the \textbf{1) best projection line} on the training data and slow the slope
		and intercept on the title \textbf{2)colorize the data} with each class \textbf{3) project
		all data points on your projection line}.}
\section{Questions}
	\subsection{Show that maximization of the class separation criterion given by 
		$L(\lambda, w) = w^{T}(m_{2} - m_{1})  + \lambda(w^{T}w - 1)$ with respect to w, using a
		Lagrange multiplier to enforce the constraint $w^{T}w = 1$, leads to the result that 
		$w \propto (m_{2} - m_{1})$.}

		\textbf{Lagrange Multipliers} allow us to maximize a function $L$ given a constraint $g$.
		To do this, we have  the initial equation $\nabla L(w, \lambda) = \alpha \nabla g(w, \lambda)$. Where 
		$\alpha$ is the Lagrange Multiplier in this equation. Since we apply the multiplier to each
		variable individually, then we have to find the multiplier based on the component functions 
		for $L$ and $g$. Thus we have to solve the system of equations 
		\[ L_{\lambda} = \alpha g_{\lambda} \quad\quad L_{w} = \alpha g_{w} \]
		So, we have to differentiate with respect to $\lambda$ and $w$, since we are dealing with the gradient
		vector. When we set up the equations, we get the system of equations
		\begin{subequations}
			\begin{equation}
				w^{T}w -1 = 0
			\end{equation}
			\begin{equation}
				\label{eq: fw}
				m_{2} - m_{1} = \alpha
			\end{equation}
			\begin{equation}
				\label{eq:constraint}
				w^{T}w = 1
			\end{equation}
		\end{subequations}
		Then, multiplying (\ref{eq:constraint}) by (\ref{eq: fw}), we get
		\[(m_{2} - m_{1})(w^{T}w) = \alpha\]
		which, when divided by $m_{2} - m_{1}$ gives 
		\[w^{T}w = \frac{\alpha}{m_{2} - m_{1}}\]

		Since $w_{T}w$ is proportional to $m_{2} - m_{1}$, then $w$ will also be. 
		Thus, using Lagrange Multipliers we have shown that $w \propto m_{2} - m_{1}$ 
	%second question
	\subsection{Using eq \ref{eq:eq1} and eq \ref{eq:eq2}, derive the result eq \ref{eq:eq3} for the posterior class
		probability in the two-class generative model with Gaussian densities, and verify the
		results eq \ref{eq:eq4} and eq \ref{eq:eq5} for the parameters w and $w_{0}$.}
		\begin{equation}
			\label{eq:eq1}
			\frac{1}{1 + exp(-\alpha) = \sigma(\alpha)}
		\end{equation}
		\begin{equation}
			\label{eq:eq2}
			a = ln\frac{p(x|C_{1})p(C_{1})}{p(x|C_{2})p(C_{2})}	
		\end{equation}
		\begin{equation}
			\label{eq:eq3}
			p(C_{1}|x) = \sigma(w^{T}x + w_{0})
		\end{equation}
		\begin{equation}
			\label{eq:eq4}
			w = \sum_{}^{-1}(\mu_{1} - \mu_{2})
		\end{equation}
		\begin{equation}
			\label{eq:eq5}
				w_{0} = -\frac{1}{2}\mu^{T}_{1}\sum_{}^{}^{-1}\mu_{1} + \frac{1}{2}\mu_{2}^{T}\sum_{}^{}^{-1}\mu_{2} + ln\frac{p(C_{1})}{p(C_{2})}
		\end{equation}
		(Eq \ref{eq:eq1}) refers to the \textbf{sigmoid function}, which moves the entire spectrum of real 
		numbers into the range $(0,1)$. $\alpha$ here refers to the ratio of the two likelihoods, which is the 
		equivalent as taking $p(x|C_{1})p(C_{1})$ and dividing it by the sum of the probabilities of the two
		classes. $w_{T}x + w{0}$ will give its relation to the ratio described by $\alpha$. 
		$\sigma(w^{T}x + w_{0})$ will thus describe the probability that the input belongs to either $C_{1}$ or 
		$C_{2}$ depending on where it lies along the $y$ axis. 
\end{document}
