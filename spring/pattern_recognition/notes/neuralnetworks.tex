\documentclass{tufte-handout}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\title{Neural Networks}
\author{Andresito Ponce}

\begin{document}
\maketitle

\begin{abstract}
	The general idea behind neural networks is to 
	maintain a \textbf{basis function} whose parameters
	can change during testing. Then the function can know 
	how to perform "better" as the testing goes on.
\end{abstract}

Previously, we had only considered a linear combination of a fixed
basis function, which usually took the form
\[ y(x, w) = f(\sum_{j=1}^{M}w_{j}\phi_{j}(x))\]

However, our goal is to make some sort of these functions depend on
certain parameters.
\[ a_{j} = \sum_{i=1}^{D}w^{(1)}_{ji}x_{i} + w^{(1)}_{j0}\]
where $a_{j}$ is the \textbf{activation} value.\footnote{Remember the
activation is the final output of this node. In classification, it 
determines which class we eventually assign to an input. The (1) 
superscript indicates they are the first layer of the NN.}

We then transform the activation value by another nonlinear function
$h$, and are left with the final result $z_{j} = h(a_{j})$. 

To summarize the basic structure: supposing the result fits into 
$K$ classes, we then sum the weights of the D nodes in the first
layer and the M nodes of the second layer, and we get a formula 
in the end resembling\footnote{This equation basically says that
the final value of class $k$ is the result of adding all the nodes
from all the layers that feed into k for the two levels?}
\[ y_{k}(x, w) = \phi(\sum_{j=1}^{M}w_{kj}^{(2)}h(\sum_{i=1}^{D}
w_{ji}^{(1)}x_{i}+w_{j0}^{(1)})+w_{k0}^{(2)})\]

\end{document}
